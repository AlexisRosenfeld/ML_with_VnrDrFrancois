{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bd2744-15ab-4d84-b8e8-2e6188ddbb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T10:23:34.302618Z",
     "iopub.status.busy": "2024-10-16T10:23:34.300185Z",
     "iopub.status.idle": "2024-10-16T10:23:34.322774Z",
     "shell.execute_reply": "2024-10-16T10:23:34.318093Z",
     "shell.execute_reply.started": "2024-10-16T10:23:34.302528Z"
    }
   },
   "source": [
    "## Machine Learning project - Meta\n",
    "\n",
    "#### Data generation\n",
    "\n",
    "A given dataset 'D' is made of a vector 'y' containing the independent variable and a matrix 'x' of dependent variables for it. D = y+x.\n",
    "\n",
    "In our case, we must distinguish 'fake' datasets 'fD', used to test our dependent variables on actual hyperparameters, and our 'real' dataset 'rD' on which our model will run. rD is derived from sets of fDs. \n",
    "\n",
    "To generate an fD = y+x we:\n",
    "1. build the vector 'y' using 'get_y()'\n",
    "2. build a matrix 'x' using 'get_x()'\n",
    "\n",
    "To generate an rD = ry+xy we:\n",
    "\n",
    "1. build a fake dataset fD using 'gen_dataset()' (that relies on the above)\n",
    "2. get the hyperparameter value for that dataset using 'get_hp()'\n",
    "3. get the dependent variables for that dataset using 'get_dv()'\n",
    "4. repeat for each line of rD\n",
    "\n",
    "Steps (1-2) are only there to confirm the impact of our dependent variables on the choice of hyperparameter. It will eventually be replaced with simple generation formulas based on our assumptions and observations.\n",
    "\n",
    "The function 'w()' lets us save our generated data as a 'csv'. We don't use 'np.savetxt()' or 'np.to_csv()' so that we can easily append. Conversely, 'w()' adds a header that should be removed if imported directly into numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e9b66e-8d7c-4a0f-8e03-9ba1bfb6298c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T05:48:55.121743Z",
     "iopub.status.busy": "2024-10-17T05:48:55.118663Z",
     "iopub.status.idle": "2024-10-17T05:48:55.166373Z",
     "shell.execute_reply": "2024-10-17T05:48:55.164158Z",
     "shell.execute_reply.started": "2024-10-17T05:48:55.121590Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier as sk_kNN\n",
    "from sklearn.metrics import accuracy_score as sk_acc\n",
    "from sklearn.model_selection import train_test_split as sk_tts\n",
    "\n",
    "## Utility functions\n",
    "def w(data: np.array, f: str=\"metadb.csv\", \n",
    "      mode: str='a', sep: str=',', ch_header: bool=True) -> None:\n",
    "    \"\"\"Writes/appends to a csv.\"\"\"\n",
    "    if not mode in ['w', 'a']:                  # invalid mode\n",
    "        return\n",
    "    if not os.path.isfile(f) or mode == 'w':    # renew header\n",
    "        wf = open(f, 'w', encoding=\"utf-8\")\n",
    "        if ch_header:\n",
    "            wf.write(sep.join([f\"x{a}\" if a > 0 else \"y\" \n",
    "                         for a in range(data.shape[1])])) # columns\n",
    "        wf.close()\n",
    "    with open(f, 'a', encoding=\"utf-8\") as wf:  # actual writing\n",
    "        for line in data:\n",
    "            ltxt = [f\"{d}\" for d in line]\n",
    "            wf.write(\"\\n\"+sep.join(ltxt))\n",
    "\n",
    "## Fake datasets\n",
    "def get_y(yc: int=3, ny: list=[100, 20]) -> np.array:\n",
    "    \"\"\"Generates an 'y' vector (fake dataset).\"\"\"\n",
    "    ly = int(np.random.normal(ny[0], ny[1])) # nb of rows\n",
    "    return np.random.choice([a for a in range(yc)], ly)\n",
    "def get_x(y: np.array, nx: list=[3, 0], var: list=[50, 5]) -> np.array:\n",
    "    \"\"\"Generates an 'x' matrix for a given 'y' vector (fake dataset).\"\"\"\n",
    "    lx = int(np.random.normal(nx[0], nx[1]))\n",
    "    mx = np.random.randint(lx)\n",
    "    x = [[np.random.normal(0, var[1])*(ry+1) if i != mx else \n",
    "          np.random.normal(var[0], var[1])*(ry+1) for i in range(lx)] \n",
    "         for ry in y]\n",
    "    return np.array(x)\n",
    "def gen_dataset(fy=None, fx=None, \n",
    "                argy: list=[3, [100, 20]],\n",
    "                argx: list=[[3, 0], [50, 5]]) -> tuple:\n",
    "    \"\"\"Generates a fake dataset.\n",
    "    Parameters:\n",
    "    - fy/fx        functions to generate y/x respectively.\n",
    "    - argy/argx    arguments for those functions, as lists.\n",
    "    Note: 'fx' will always receive 'y' as first parameter.\"\"\"\n",
    "    fy, fx = get_y if not fy else fy, get_x if not fx else fx\n",
    "    y = fy(*argy)\n",
    "    return (y, fx(y, *argx))\n",
    "\n",
    "## Hyperparameter / dependent variables\n",
    "def get_hp(y: np.array, x: np.array) -> int:\n",
    "    \"\"\"Runs a kNN to find the best 'k'.\"\"\"\n",
    "    x_tr, x_te, y_tr, y_te = sk_tts(x, y) # train/test\n",
    "    rk, ra, ly = 0, -1., int(len(y)/2)-1\n",
    "    for k in np.arange(1, ly, 1):         # all possible k's\n",
    "        m = sk_kNN(k)                     # kNN model\n",
    "        m.fit(x_tr, y_tr)                 # fit on training data\n",
    "        yp_te = m.predict(x_te)           # predict on test data\n",
    "        a = sk_acc(y_te, yp_te)           # get accuracy score\n",
    "        if a > ra:                        # pick best 'k'\n",
    "            ra, rk = a, k\n",
    "    return rk\n",
    "def get_dv(y: np.array, x: np.array) -> np.array:\n",
    "    \"\"\"Gets dependent variables from our y/x dataset.\"\"\"\n",
    "    # add variables...\n",
    "    rx = [len(y),                         # nb of rows\n",
    "          len(np.unique(y)),              # nb of classes\n",
    "          len(x[0]),                      # nb of dependent variables\n",
    "    ]\n",
    "    return np.array(rx)\n",
    "\n",
    "## Main function\n",
    "def simulate(n: int=1000, fy=None, fx=None, \n",
    "             argy: list=[3, [100, 20]],\n",
    "             argx: list=[[3, 0], [50, 5]]) -> np.array:\n",
    "    \"\"\"Main function to generate 'n' datapoints for our model.\n",
    "    Parameters:\n",
    "    - n          number of fake datasets.\n",
    "    - fx/fy      functions to generate y/x respectively (fake datasets)\n",
    "    - argy/argx  arguments for those functions, as lists.\n",
    "    Returns:\n",
    "    - a numpy matrix with hyperparameter as first column \n",
    "                          and and dependent variables as the rest.\"\"\"\n",
    "    res = []                           # return value (matrix)\n",
    "    for _ in range(n):\n",
    "        y, x = gen_dataset(fy, fx, argy, argx) # fake dataset\n",
    "        # s = time.time()\n",
    "        ry = get_hp(y, x)              # hyperparameter\n",
    "        # print(\"HP:\", _, time.time()-s)\n",
    "        rx = get_dv(y, x)              # dependent variables\n",
    "        res.append([ry]+list(rx))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2e015-1d9c-433b-b731-0502ce51f8d8",
   "metadata": {},
   "source": [
    "#### Dataset length\n",
    "\n",
    "As a rule of thumb, the more data we have the higher the hyperparameter 'k'. The rule of thumb is $\\sqrt{N}/2$ where N is the number of datapoints. \n",
    "\n",
    "We can verify this (below) by simulating datasets of length 100 and 1000 respectively. Testing the variance is pointless as, eventually, we get the same average. \n",
    "* For 100 datapoints, we get k ~= 2 (rule of thumb ~5).\n",
    "* For 1000 datapoints, we get k ~= 10 (rule of thumb ~15).\n",
    "Note: getting the hyperparameter for 1000 datapoints, all else equal, takes ~14s. Testing for 10,000 datapoints would become prohibitive.\n",
    "\n",
    "For simulation purposes we can assume that the rule of thumb stands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5468b49-8966-43fc-be72-7fc76f5a9fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T11:49:38.906054Z",
     "iopub.status.busy": "2024-10-16T11:49:38.903884Z",
     "iopub.status.idle": "2024-10-16T11:49:38.929484Z",
     "shell.execute_reply": "2024-10-16T11:49:38.927681Z",
     "shell.execute_reply.started": "2024-10-16T11:49:38.905897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[100, 5]    5.9  Time: 1.998\\n   [1000, 5]   13.2 Time: 144.840\\n   [100, 40]   1.7  Time: 2.284\\n   [1000, 100] 11.4 Time: 151.673'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_len():\n",
    "    \"\"\"Temporary function to test hyperparameter variation, \n",
    "       here data length.\"\"\"\n",
    "    for i, ny in enumerate([[100, 5], [1000, 5], [100, 40], [1000, 100]]):\n",
    "        s = time.time()\n",
    "        a = simulate(10, argy=[3, ny])# ; w(a, mode=\"w\" if i == 0 else \"a\")\n",
    "        print(ny, np.mean(a[:, 0]), f\"Time: {time.time()-s:.03f}\")\n",
    "\n",
    "#test_len()\n",
    "\"\"\"[100, 5]    5.9  Time: 1.998\n",
    "   [1000, 5]   13.2 Time: 144.840\n",
    "   [100, 40]   1.7  Time: 2.284\n",
    "   [1000, 100] 11.4 Time: 151.673\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4e2b8-c734-4463-a030-94741cf4b62b",
   "metadata": {},
   "source": [
    "#### Dataset noise\n",
    "\n",
    "The main reason to pick a higher 'k' is for noisy data. The only way I can think of simulating that is adding variance to our 'x'. But testing is inconclusive: a higher variance doesn't lead to a higher 'k'. ('k' values vary widely from run to run, any pattern is illusory. What matters is there is no linear relation.)\n",
    "\n",
    "We need another way to simulate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56058d7b-c936-48c5-be5a-cf735b761c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T06:12:52.452748Z",
     "iopub.status.busy": "2024-10-17T06:12:52.451778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 14.7\n",
      "Linear: 1.0\n",
      "Quadratic: 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_rand_x(y):\n",
    "    \"\"\"No relationship with 'y' whatsoever.\"\"\"\n",
    "    lx = 3 # nb of columns\n",
    "    return np.array([[np.random.normal(0, 100) for i in range(lx)] \n",
    "                                               for ry in y])\n",
    "def get_lin_x(y):\n",
    "    \"\"\"Same as 'get_x'.\"\"\"\n",
    "    lx = 3                      # nb of columns\n",
    "    mx = np.random.randint(lx)  # the 'x' that actually impacts 'y'\n",
    "    return np.array([[np.random.normal(0, 30) if i != mx else \n",
    "                      np.random.normal(1000, 5)*(ry+1) for i in range(lx)] \n",
    "                                                       for ry in y])\n",
    "def get_qu_x(y):\n",
    "    \"\"\"Quadratic relationship.\"\"\"\n",
    "    lx = 3                      # nb of columns\n",
    "    mx = np.random.randint(lx)  # the 'x' that actually impacts 'y'\n",
    "    return np.array([[np.random.normal(0, 30) if i != mx else \n",
    "                      (np.random.normal(1000, 5)*(ry+1))**2 \n",
    "                      for i in range(lx)] for ry in y])\n",
    "def get_maybe_x(y):\n",
    "    \"\"\"Relationship is not absolute.\"\"\"\n",
    "    lx = 3                      # nb of columns\n",
    "    mx = np.random.randint(lx)  # the 'x' that actually impacts 'y'\n",
    "    p = 0.2                     # probability of failing to impact 'y'\n",
    "    res = []\n",
    "    for ry in y:\n",
    "        res.append([])\n",
    "        for i in range(lx):\n",
    "            if i != mx:         # no relationship\n",
    "                res[-1].append(np.random.normal(0, 30)); continue\n",
    "            ch = np.random.choice([0, 1], p=[p, 1-p]) # maybe none?\n",
    "            res[-1].append(np.random.normal(1000, 5)*(ry+1)\n",
    "                           if ch == 1 else np.random.normal(0, 30))\n",
    "    return np.array(res)\n",
    "\n",
    "def test_x_noise():\n",
    "    \"\"\"Temporary function to test hyperparameter variation, \n",
    "       here data noise (relying on 'x' variance).\"\"\"\n",
    "    a = simulate(10, fx=get_rand_x, argx=[]); w(a, mode=\"w\")\n",
    "    print(\"Random:\", np.mean(a[:, 0]))\n",
    "    a = simulate(10, fx=get_lin_x, argx=[]); w(a)\n",
    "    print(\"Linear:\", np.mean(a[:, 0]))\n",
    "    a = simulate(10, fx=get_qu_x, argx=[]); w(a)\n",
    "    print(\"Quadratic:\", np.mean(a[:, 0]))\n",
    "    a = simulate(10, fx=get_maybe_x, argx=[]); w(a)\n",
    "    print(\"Linear-probability:\", np.mean(a[:, 0]))\n",
    "\n",
    "test_x_noise() # test on 'x' distributions\n",
    "\"\"\"\n",
    "    Random: 7.1               # expected (but low?)\n",
    "    Linear: 1.0\n",
    "    Quadratic: 1.0            # failed simulation?\n",
    "    Linear-probability: 5.4   # only lack of relation?\n",
    "\"\"\"\n",
    "\n",
    "#test_x_noise() # old test on 'x' variance\n",
    "\"\"\"[1000, 10]  1.0 Time: 1.958\n",
    "   [1000, 50]  1.0 Time: 1.753\n",
    "   [1000, 100] 1.1 Time: 2.120\n",
    "   [1000, 200] 3.9 Time: 2.038\n",
    "   [1000, 300] 4.8 Time: 1.959\n",
    "   [1000, 400] 9.3 Time: 1.826\n",
    "   [1000, 500] 6.4 Time: 2.250\n",
    "   [1000, 800] 3.2 Time: 1.812\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_ml",
   "language": "python",
   "name": "py3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
